{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-15T01:19:10.437278Z",
     "iopub.status.busy": "2025-10-15T01:19:10.437038Z",
     "iopub.status.idle": "2025-10-15T01:19:13.501761Z",
     "shell.execute_reply": "2025-10-15T01:19:13.500740Z",
     "shell.execute_reply.started": "2025-10-15T01:19:10.437249Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OVERVIEW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Title:\n",
    "Predicting Bitcoin Price Movement Using Twitter Sentiment and Machine Learning\n",
    "\n",
    "Objective:\n",
    "The goal of this project is to determine whether aggregated Twitter sentiment about Bitcoin can be used to forecast its price movement over the next 24 hours. Cryptocurrencies like Bitcoin are highly volatile and influenced heavily by investor sentiment, which makes traditional financial models less effective.\n",
    "We go through the tweets made about the bitcoins in twitter. and predict bitcoin price based on tweet's sentiment. For positive, price will increase. if negative viceversa.\n",
    "\n",
    "Data Sources:\n",
    "\n",
    "1.Historical Bitcoin Price Data: Provides the ground truth for price movement (Open, Close, High, Low, Volume).   [](http://)https://www.kaggle.com/datasets/mohammednawazkagle04/bitcoins-history-data\n",
    "\n",
    "2.Twitter Sentiment Data: Tweets mentioning Bitcoin, analyzed to compute sentiment scores (positive, negative, neutral).\n",
    "[](http://)https://www.kaggle.com/datasets/mohammednawazkagle04/btc-tweets-sentiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATASETS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T01:19:13.504228Z",
     "iopub.status.busy": "2025-10-15T01:19:13.503806Z",
     "iopub.status.idle": "2025-10-15T01:19:13.637840Z",
     "shell.execute_reply": "2025-10-15T01:19:13.634514Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.504202Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bitcoin price dataset contains:\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/bitcoins-history-data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_37/755373693.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" Bitcoin price dataset contains:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/bitcoins-history-data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n Tweet sentiment dataset contains:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/bitcoins-history-data'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\" Bitcoin price dataset contains:\")\n",
    "print(os.listdir(\"/kaggle/input/bitcoins-history-data\"))\n",
    "\n",
    "print(\"\\n Tweet sentiment dataset contains:\")\n",
    "print(os.listdir(\"/kaggle/input/btc-tweets-sentiment\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.638330Z",
     "iopub.status.idle": "2025-10-15T01:19:13.638645Z",
     "shell.execute_reply": "2025-10-15T01:19:13.638531Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.638518Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "btc_price = pd.read_csv('/kaggle/input/bitcoins-history-data/btcusd_1-min_data.csv')\n",
    "tweets = pd.read_csv('/kaggle/input/btc-tweets-sentiment/BTC_Tweets_Updated.csv')\n",
    "\n",
    "print(\"BTC Price Data:\", btc_price.shape)\n",
    "print(\"Tweets Data:\", tweets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.639649Z",
     "iopub.status.idle": "2025-10-15T01:19:13.639926Z",
     "shell.execute_reply": "2025-10-15T01:19:13.639818Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.639806Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"BTC Price Columns:\\n\", btc_price.columns)\n",
    "print(\"\\nTweets Columns:\\n\", tweets.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.640983Z",
     "iopub.status.idle": "2025-10-15T01:19:13.641705Z",
     "shell.execute_reply": "2025-10-15T01:19:13.641520Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.641498Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tweets['Date'] = pd.to_datetime(tweets['Date'], format='%a %b %d %H:%M:%S %z %Y', errors='coerce')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATE AND TIME CONVERSION:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.642631Z",
     "iopub.status.idle": "2025-10-15T01:19:13.643112Z",
     "shell.execute_reply": "2025-10-15T01:19:13.642990Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.642976Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# BTC dataset\n",
    "btc_price.columns = ['Timestamp','Open','High','Low','Close','Volume']\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "btc_price['Date'] = pd.to_datetime(btc_price['Timestamp'], unit='s')\n",
    "btc_price.drop(columns=['Timestamp'], inplace=True)\n",
    "\n",
    "# Tweets dataset\n",
    "tweets.columns = ['id','Date','Tweet','Screen_name','Source','Link','Sentiment','sent_score','New_Sentiment_Score','New_Sentiment_State','BERT_Labels']\n",
    "\n",
    "# Convert Twitter dates to datetime\n",
    "tweets['Date'] = pd.to_datetime(tweets['Date'], format='%a %b %d %H:%M:%S %z %Y', errors='coerce')\n",
    "tweets['Date'] = tweets['Date'].dt.tz_localize(None)  # remove timezone\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.643837Z",
     "iopub.status.idle": "2025-10-15T01:19:13.644166Z",
     "shell.execute_reply": "2025-10-15T01:19:13.644015Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.643999Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check BTC dataset\n",
    "print(btc_price.head())\n",
    "\n",
    "# Check Tweets dataset\n",
    "print(tweets.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.646221Z",
     "iopub.status.idle": "2025-10-15T01:19:13.646672Z",
     "shell.execute_reply": "2025-10-15T01:19:13.646507Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.646489Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Assuming your BTC dataset is named btc_price\n",
    "btc_price.columns = ['Timestamp','Open','High','Low','Close','Volume']\n",
    "\n",
    "# Convert epoch seconds to datetime\n",
    "btc_price['Date'] = pd.to_datetime(btc_price['Timestamp'], unit='s')\n",
    "\n",
    "# Drop old Timestamp column\n",
    "btc_price.drop(columns=['Timestamp'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " CREATES DAILY SUMMARY OF DATA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.648415Z",
     "iopub.status.idle": "2025-10-15T01:19:13.648777Z",
     "shell.execute_reply": "2025-10-15T01:19:13.648623Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.648606Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Parse tweet dates if not done\n",
    "tweets['Date'] = pd.to_datetime(tweets['Date'], format='%a %b %d %H:%M:%S %z %Y', errors='coerce')\n",
    "tweets['Date'] = tweets['Date'].dt.tz_localize(None)  # remove timezone\n",
    "\n",
    "# Aggregate tweets per day\n",
    "tweet_daily = tweets.groupby(tweets['Date'].dt.date).agg({\n",
    "    'New_Sentiment_Score':'mean',          # average sentiment per day\n",
    "    'Tweet':'count',                        # total tweets per day\n",
    "    'BERT_Labels': lambda x: (x==1).sum()  # count positive tweets\n",
    "}).rename(columns={'Tweet':'Total_Tweets','BERT_Labels':'Positive_Tweets'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.649909Z",
     "iopub.status.idle": "2025-10-15T01:19:13.650302Z",
     "shell.execute_reply": "2025-10-15T01:19:13.650085Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.650068Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load BTC Price dataset\n",
    "btc = pd.read_csv(\"/kaggle/input/bitcoins-history-data/btcusd_1-min_data.csv\")\n",
    "\n",
    "# Load Tweet Sentiment dataset\n",
    "tweets = pd.read_csv(\"/kaggle/input/btc-tweets-sentiment/BTC_Tweets_Updated.csv\")\n",
    "\n",
    "print(\" BTC Data Shape:\", btc.shape)\n",
    "print(\" Tweets Data Shape:\", tweets.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE TO LOAD AND USES BERT POSITIVES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.651252Z",
     "iopub.status.idle": "2025-10-15T01:19:13.651617Z",
     "shell.execute_reply": "2025-10-15T01:19:13.651487Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.651474Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1️ Load datasets\n",
    "btc = pd.read_csv(\"/kaggle/input/bitcoins-history-data/btcusd_1-min_data.csv\")\n",
    "tweets = pd.read_csv(\"/kaggle/input/btc-tweets-sentiment/BTC_Tweets_Updated.csv\")\n",
    "\n",
    "#  2️ Fix BTC timestamps (assuming UNIX milliseconds; change 'ms' to 's' if needed)\n",
    "btc['Timestamp'] = pd.to_datetime(btc['Timestamp'], unit='ms', errors='coerce')\n",
    "\n",
    "# Drop invalid timestamps\n",
    "btc = btc.dropna(subset=['Timestamp'])\n",
    "\n",
    "# Aggregate BTC to daily\n",
    "btc_daily = (\n",
    "    btc.groupby(btc['Timestamp'].dt.date)\n",
    "    .agg({\n",
    "        'Open': 'first',\n",
    "        'High': 'max',\n",
    "        'Low': 'min',\n",
    "        'Close': 'last',\n",
    "        'Volume': 'sum'\n",
    "    })\n",
    "    .reset_index()\n",
    "    .rename(columns={'Timestamp': 'Date'})\n",
    ")\n",
    "\n",
    "#  Process tweet dates\n",
    "tweets['Date'] = pd.to_datetime(tweets['Date'], format='%a %b %d %H:%M:%S %z %Y', errors='coerce')\n",
    "tweets['Date'] = tweets['Date'].dt.tz_localize(None)\n",
    "\n",
    "# Aggregate tweets daily\n",
    "tweet_daily = (\n",
    "    tweets.groupby(tweets['Date'].dt.date)\n",
    "    .agg({\n",
    "        'New_Sentiment_Score': 'mean',\n",
    "        'Tweet': 'count',\n",
    "        'BERT Labels': lambda x: (x == 1).sum()\n",
    "    })\n",
    "    .rename(columns={'Tweet': 'Total_Tweets', 'BERT Labels': 'Positive_Tweets'})\n",
    "    .reset_index()\n",
    "    .rename(columns={'Date': 'Tweet_Date'})\n",
    ")\n",
    "\n",
    "#  4 Convert both dates to date-only for merging\n",
    "btc_daily['Date'] = pd.to_datetime(btc_daily['Date']).dt.date\n",
    "tweet_daily['Tweet_Date'] = pd.to_datetime(tweet_daily['Tweet_Date']).dt.date\n",
    "\n",
    "#  5️ Merge BTC and tweet sentiment (left join keeps all BTC dates)\n",
    "merged = pd.merge(\n",
    "    btc_daily, tweet_daily,\n",
    "    left_on='Date', right_on='Tweet_Date',\n",
    "    how='left'\n",
    ")\n",
    "merged.drop(columns=['Tweet_Date'], inplace=True)\n",
    "\n",
    "# Fill missing tweet sentiment with 0\n",
    "merged[['New_Sentiment_Score', 'Total_Tweets', 'Positive_Tweets']] = merged[\n",
    "    ['New_Sentiment_Score', 'Total_Tweets', 'Positive_Tweets']\n",
    "].fillna(0)\n",
    "\n",
    "#  6️ Create target variable\n",
    "merged['Next_Close'] = merged['Close'].shift(-1)\n",
    "merged['Price_Change'] = merged['Next_Close'] - merged['Close']\n",
    "merged = merged.dropna(subset=['Close', 'Next_Close', 'Price_Change'])\n",
    "merged['Target'] = (merged['Price_Change'] > 0).astype(int)\n",
    "\n",
    "print(\"Merged dataset ready:\", merged.shape)\n",
    "print(merged.head())\n",
    "\n",
    "#  7️ Train-test split\n",
    "X = merged[['Open', 'High', 'Low', 'Close', 'Volume', \n",
    "            'New_Sentiment_Score', 'Total_Tweets', 'Positive_Tweets']]\n",
    "y = merged['Target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "#  8️ Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.652765Z",
     "iopub.status.idle": "2025-10-15T01:19:13.653191Z",
     "shell.execute_reply": "2025-10-15T01:19:13.652955Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.652940Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "btc = pd.read_csv(\"/kaggle/input/bitcoins-history-data/btcusd_1-min_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.654306Z",
     "iopub.status.idle": "2025-10-15T01:19:13.654620Z",
     "shell.execute_reply": "2025-10-15T01:19:13.654508Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.654496Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(os.listdir(\"/kaggle/input/btc-tweets-sentiment\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.656061Z",
     "iopub.status.idle": "2025-10-15T01:19:13.656452Z",
     "shell.execute_reply": "2025-10-15T01:19:13.656224Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.656209Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"/kaggle/input/btc-tweets-sentiment/BTC_Tweets_Updated.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.657760Z",
     "iopub.status.idle": "2025-10-15T01:19:13.658055Z",
     "shell.execute_reply": "2025-10-15T01:19:13.657929Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.657914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.658746Z",
     "iopub.status.idle": "2025-10-15T01:19:13.659086Z",
     "shell.execute_reply": "2025-10-15T01:19:13.658893Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.658881Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "bitcoin_df = pd.read_csv(\"/kaggle/input/bitcoins-history-data/btcusd_1-min_data.csv\")\n",
    "tweets_df = pd.read_csv(\"/kaggle/input/btc-tweets-sentiment/BTC_Tweets_Updated.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.659756Z",
     "iopub.status.idle": "2025-10-15T01:19:13.660043Z",
     "shell.execute_reply": "2025-10-15T01:19:13.659936Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.659921Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.661320Z",
     "iopub.status.idle": "2025-10-15T01:19:13.661926Z",
     "shell.execute_reply": "2025-10-15T01:19:13.661752Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.661734Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load Bitcoin price data\n",
    "bitcoin_df = pd.read_csv(\"/kaggle/input/bitcoins-history-data/btcusd_1-min_data.csv\")\n",
    "\n",
    "# Load Tweets sentiment data\n",
    "tweets_df = pd.read_csv(\"/kaggle/input/btc-tweets-sentiment/BTC_Tweets_Updated.csv\")\n",
    "\n",
    "print(\"Bitcoin data shape:\", bitcoin_df.shape)\n",
    "print(\" Tweets data shape:\", tweets_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONVERTION OF DATA COLUMNS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.662908Z",
     "iopub.status.idle": "2025-10-15T01:19:13.663186Z",
     "shell.execute_reply": "2025-10-15T01:19:13.663060Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.663047Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Rename Timestamp → Date for Bitcoin data\n",
    "bitcoin_df.rename(columns={'Timestamp': 'Date'}, inplace=True)\n",
    "\n",
    "# Convert both to datetime\n",
    "bitcoin_df['Date'] = pd.to_datetime(bitcoin_df['Date'], errors='coerce')\n",
    "tweets_df['Date'] = pd.to_datetime(tweets_df['Date'], errors='coerce')\n",
    "\n",
    "print(\" Date columns converted successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.664330Z",
     "iopub.status.idle": "2025-10-15T01:19:13.664718Z",
     "shell.execute_reply": "2025-10-15T01:19:13.664517Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.664502Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tweets_daily = tweets_df.groupby(tweets_df['Date'].dt.date).agg({\n",
    "    'New_Sentiment_Score': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "tweets_daily.rename(columns={'Date': 'Date', 'New_Sentiment_Score': 'Avg_Sentiment'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.666265Z",
     "iopub.status.idle": "2025-10-15T01:19:13.666623Z",
     "shell.execute_reply": "2025-10-15T01:19:13.666505Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.666489Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Bitcoin daily close\n",
    "bitcoin_daily = bitcoin_df.groupby(bitcoin_df['Date'].dt.date).agg({'Close':'last','Volume':'sum'}).reset_index()\n",
    "\n",
    "# Tweets daily average sentiment\n",
    "tweets_daily = tweets_df.groupby(tweets_df['Date'].dt.date).agg({'New_Sentiment_Score':'mean'}).reset_index()\n",
    "tweets_daily.rename(columns={'New_Sentiment_Score':'Avg_Sentiment'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.667713Z",
     "iopub.status.idle": "2025-10-15T01:19:13.668004Z",
     "shell.execute_reply": "2025-10-15T01:19:13.667881Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.667866Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "merged_df = pd.merge(bitcoin_daily, tweets_daily, on='Date', how='inner')\n",
    "merged_df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.668948Z",
     "iopub.status.idle": "2025-10-15T01:19:13.669339Z",
     "shell.execute_reply": "2025-10-15T01:19:13.669122Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.669107Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "merged_df['Next_Close'] = merged_df['Close'].shift(-1)\n",
    "merged_df['Target'] = (merged_df['Next_Close'] > merged_df['Close']).astype(int)\n",
    "merged_df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMPLIFY THE DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE AGAIN , WHICH IS EASIER FOR DEMONSTRATION AND DEBUGGING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.670582Z",
     "iopub.status.idle": "2025-10-15T01:19:13.670904Z",
     "shell.execute_reply": "2025-10-15T01:19:13.670759Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.670743Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Step 1: Load datasets\n",
    "\n",
    "bitcoin_df = pd.read_csv(\"/kaggle/input/bitcoins-history-data/btcusd_1-min_data.csv\")\n",
    "tweets_df = pd.read_csv(\"/kaggle/input/btc-tweets-sentiment/BTC_Tweets_Updated.csv\")\n",
    "\n",
    "# Inspect columns\n",
    "print(\"Bitcoin columns:\", bitcoin_df.columns)\n",
    "print(\"Tweets columns:\", tweets_df.columns)\n",
    "\n",
    "\n",
    "# Step 2: Convert dates\n",
    "\n",
    "# Bitcoin: convert Timestamp (Unix) to datetime\n",
    "bitcoin_df['Date'] = pd.to_datetime(bitcoin_df['Timestamp'], unit='s')\n",
    "bitcoin_df['Date'] = bitcoin_df['Date'].dt.date  # keep only YYYY-MM-DD\n",
    "\n",
    "# Tweets: convert Date to datetime\n",
    "tweets_df['Date'] = pd.to_datetime(tweets_df['Date'], errors='coerce')\n",
    "tweets_df['Date'] = tweets_df['Date'].dt.date  # keep only YYYY-MM-DD\n",
    "\n",
    "\n",
    "# Step 3: Aggregate Bitcoin to daily\n",
    "\n",
    "bitcoin_daily = bitcoin_df.groupby('Date').agg({\n",
    "    'Close': 'last',  # last price of the day\n",
    "    'Volume': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "# Step 4: Aggregate Tweets to daily sentiment\n",
    "\n",
    "tweets_daily = tweets_df.groupby('Date').agg({\n",
    "    'New_Sentiment_Score': 'mean'  # average sentiment per day\n",
    "}).reset_index()\n",
    "tweets_daily.rename(columns={'New_Sentiment_Score':'Avg_Sentiment'}, inplace=True)\n",
    "\n",
    "\n",
    "# Step 5: Merge datasets safely\n",
    "\n",
    "merged_df = pd.merge(bitcoin_daily, tweets_daily, on='Date', how='outer')\n",
    "\n",
    "# Fill missing sentiment with 0 (no tweets that day)\n",
    "merged_df['Avg_Sentiment'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# Step 6: Create target variable\n",
    "\n",
    "merged_df['Next_Close'] = merged_df['Close'].shift(-1)\n",
    "merged_df['Target'] = (merged_df['Next_Close'] > merged_df['Close']).astype(int)\n",
    "\n",
    "# Drop last row (no next close)\n",
    "merged_df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "# Step 7: Check merged data\n",
    "\n",
    "print(\"Merged DataFrame shape:\", merged_df.shape)\n",
    "print(merged_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  BALANCES AND TRAIN THE DATASET:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.671929Z",
     "iopub.status.idle": "2025-10-15T01:19:13.672423Z",
     "shell.execute_reply": "2025-10-15T01:19:13.672182Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.672164Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate classes\n",
    "df_majority = merged_df[merged_df.Target == 1]\n",
    "df_minority = merged_df[merged_df.Target == 0]\n",
    "\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(\n",
    "    df_minority, \n",
    "    replace=True,     # sample with replacement\n",
    "    n_samples=len(df_majority),  # match majority count\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Combine back\n",
    "balanced_df = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "# Split again\n",
    "X = balanced_df[['Close', 'Volume', 'Avg_Sentiment']]\n",
    "y = balanced_df['Target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.673351Z",
     "iopub.status.idle": "2025-10-15T01:19:13.673628Z",
     "shell.execute_reply": "2025-10-15T01:19:13.673519Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.673508Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = merged_df[['Close','Volume','Avg_Sentiment']]\n",
    "y = merged_df['Target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUVATION CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.674957Z",
     "iopub.status.idle": "2025-10-15T01:19:13.675213Z",
     "shell.execute_reply": "2025-10-15T01:19:13.675101Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.675090Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "# 1️ Balance dataset\n",
    "df_majority = merged_df[merged_df.Target == 1]\n",
    "df_minority = merged_df[merged_df.Target == 0]\n",
    "\n",
    "df_minority_upsampled = resample(\n",
    "    df_minority,\n",
    "    replace=True,\n",
    "    n_samples=len(df_majority),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "balanced_df = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "# 2️ Features & target\n",
    "X = balanced_df[['Close', 'Volume', 'Avg_Sentiment']]\n",
    "y = balanced_df['Target']\n",
    "\n",
    "# 3️ Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4️ Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 5️ Train model with balanced weights\n",
    "lr_model = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6️ Predictions\n",
    "y_pred = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# 7️ Check unique predictions\n",
    "print(\"Unique predictions:\", np.unique(y_pred, return_counts=True))\n",
    "\n",
    "# 8️ Evaluation\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DESCRIPTION: BERT Sentiment Labels Explanation:\n",
    "\n",
    "\n",
    "The [BERT Labels] column in our tweets dataset was generated using a pre-trained, BERT (Bidirectional Encoder Representations from Transformers) model for sentiment classification.\n",
    "\n",
    "-> Each tweet was passed through the BERT model to determine whether it expressed a [positive], [neutral], or [negative] sentiment toward Bitcoin.\n",
    "-> The output labels were then stored in the column `BERT Labels`.\n",
    "->Using these labels, we computed the \"New_Sentiment_Score\" (numerical representation of tweet sentiment) and the \"New_Sentiment_State\" (categorical representation).\n",
    "-> Finally, daily average sentiment (`Avg_Sentiment`) was calculated by aggregating tweet sentiment scores per day and merged with the Bitcoin price data.\n",
    "\n",
    "This allows us to correlate social sentiment with market movement, aiming to predict whether Bitcoin’s price will rise or fall within the next 24 hours.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  RESULTS VISUALIZATION:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.676531Z",
     "iopub.status.idle": "2025-10-15T01:19:13.676886Z",
     "shell.execute_reply": "2025-10-15T01:19:13.676728Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.676711Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(merged_df['Date'], merged_df['Close'], label='Bitcoin Price')\n",
    "plt.plot(merged_df['Date'], merged_df['Avg_Sentiment']*10000, label='Avg Sentiment (scaled)')\n",
    "plt.legend()\n",
    "plt.title(\"Bitcoin Price vs Twitter Sentiment\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FOREST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.678282Z",
     "iopub.status.idle": "2025-10-15T01:19:13.678585Z",
     "shell.execute_reply": "2025-10-15T01:19:13.678471Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.678457Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " CONFUSION MATRIX OR CLASSIFICATION REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.679239Z",
     "iopub.status.idle": "2025-10-15T01:19:13.679708Z",
     "shell.execute_reply": "2025-10-15T01:19:13.679569Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.679553Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ConfusionMatrixDisplay(cm).plot()\n",
    "plt.show()\n",
    "\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "print(\"ROC-AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " INTIAL CONCLUSION BEFORE TUNING:\n",
    "\n",
    "1.The project demonstrates that Twitter sentiment has some correlation with Bitcoin price movements, but predicting exact direction is challenging.\n",
    "\n",
    "2.Models achieved an accuracy of ~51%, showing slight predictive power above random guessing.\n",
    "\n",
    "Positive sentiment tends to be slightly more indicative of upward price movement, as reflected in recall scores.\n",
    "\n",
    "3.Feature engineering (rolling sentiment averages, price momentum) can further improve model performance.\n",
    "\n",
    "4.Limitations include noisy social media data, low signal-to-noise ratio, and highly volatile cryptocurrency market.\n",
    "\n",
    "Future improvements:\n",
    "\n",
    "Fine-tuning BERT on cryptocurrency tweets\n",
    "\n",
    "Using ensemble or deep learning models (LSTM/GRU) for sequential prediction\n",
    "\n",
    "Incorporating additional indicators like trading volume, Google Trends, or Reddit sentiment\n",
    "\n",
    "Takeaway:\n",
    "This project showcases the integration of social media sentiment analysis with financial data \n",
    "for predictive modeling, highlighting the challenges and opportunities of machine learning in volatile markets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUNING METHODS TO IMPROVE ACCURACY: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.681044Z",
     "iopub.status.idle": "2025-10-15T01:19:13.681418Z",
     "shell.execute_reply": "2025-10-15T01:19:13.681215Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.681201Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "merged_df['Return_1'] = merged_df['Close'].pct_change(1).fillna(0)\n",
    "merged_df['Return_2'] = merged_df['Close'].pct_change(2).fillna(0)\n",
    "merged_df['Return_3'] = merged_df['Close'].pct_change(3).fillna(0)\n",
    "merged_df['Sentiment_Lag1'] = merged_df['Avg_Sentiment'].shift(1).fillna(0)\n",
    "merged_df['Sentiment_Lag2'] = merged_df['Avg_Sentiment'].shift(2).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.682981Z",
     "iopub.status.idle": "2025-10-15T01:19:13.683255Z",
     "shell.execute_reply": "2025-10-15T01:19:13.683130Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.683118Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "merged_df['MA3'] = merged_df['Close'].rolling(3).mean().fillna(0)\n",
    "merged_df['MA5'] = merged_df['Close'].rolling(5).mean().fillna(0)\n",
    "merged_df['Volatility3'] = merged_df['Close'].rolling(3).std().fillna(0)\n",
    "merged_df['Volatility5'] = merged_df['Close'].rolling(5).std().fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.684102Z",
     "iopub.status.idle": "2025-10-15T01:19:13.684347Z",
     "shell.execute_reply": "2025-10-15T01:19:13.684240Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.684229Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "merged_df['Vol_Change'] = merged_df['Volume'].pct_change().fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.685863Z",
     "iopub.status.idle": "2025-10-15T01:19:13.686202Z",
     "shell.execute_reply": "2025-10-15T01:19:13.686046Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.686032Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#  Features & Target (safe copy)\n",
    "\n",
    "features = ['Close','Volume','Avg_Sentiment',\n",
    "            'Return_1','Return_2','Return_3',\n",
    "            'MA3','MA5','Volatility3','Volatility5',\n",
    "            'Sentiment_Lag1','Sentiment_Lag2','Vol_Change']\n",
    "\n",
    "X = merged_df[features].copy()  # <-- make an explicit copy\n",
    "y = merged_df['Target']\n",
    "\n",
    "# Replace inf/-inf with NaN\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# Fill NaN with 0\n",
    "X.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs feature engineering and trains an XGBoost classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.687335Z",
     "iopub.status.idle": "2025-10-15T01:19:13.687691Z",
     "shell.execute_reply": "2025-10-15T01:19:13.687539Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.687525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#  Feature Engineering for XGBoost\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Lagged returns\n",
    "merged_df['Return_1'] = merged_df['Close'].pct_change(1).fillna(0)\n",
    "merged_df['Return_2'] = merged_df['Close'].pct_change(2).fillna(0)\n",
    "merged_df['Return_3'] = merged_df['Close'].pct_change(3).fillna(0)\n",
    "\n",
    "# Lagged sentiment\n",
    "merged_df['Sentiment_Lag1'] = merged_df['Avg_Sentiment'].shift(1).fillna(0)\n",
    "merged_df['Sentiment_Lag2'] = merged_df['Avg_Sentiment'].shift(2).fillna(0)\n",
    "\n",
    "# Moving averages\n",
    "merged_df['MA3'] = merged_df['Close'].rolling(3).mean().fillna(0)\n",
    "merged_df['MA5'] = merged_df['Close'].rolling(5).mean().fillna(0)\n",
    "\n",
    "# Volatility\n",
    "merged_df['Volatility3'] = merged_df['Close'].rolling(3).std().fillna(0)\n",
    "merged_df['Volatility5'] = merged_df['Close'].rolling(5).std().fillna(0)\n",
    "\n",
    "# Volume change\n",
    "merged_df['Vol_Change'] = merged_df['Volume'].pct_change().fillna(0)\n",
    "\n",
    "\n",
    "#  Features & Target\n",
    "\n",
    "features = ['Close','Volume','Avg_Sentiment',\n",
    "            'Return_1','Return_2','Return_3',\n",
    "            'MA3','MA5','Volatility3','Volatility5',\n",
    "            'Sentiment_Lag1','Sentiment_Lag2','Vol_Change']\n",
    "\n",
    "X = merged_df[features].copy()\n",
    "y = merged_df['Target']\n",
    "\n",
    "# Replace inf/-inf and NaN\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "#  Train-Test Split (chronological)\n",
    "\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "\n",
    "#  Scaling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "#  XGBoost Classifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "#  Predictions & Evaluation\n",
    "\n",
    "y_pred = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\n Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ConfusionMatrixDisplay(cm).plot()\n",
    "plt.show()\n",
    "\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "print(\"ROC-AUC:\", auc)\n",
    "\n",
    "\n",
    "# Feature Importance\n",
    "\n",
    "importances = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop Features:\\n\", importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.688687Z",
     "iopub.status.idle": "2025-10-15T01:19:13.689013Z",
     "shell.execute_reply": "2025-10-15T01:19:13.688861Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.688847Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#  Prepare Rolling Features for XGBoost\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure merged_df from your notebook exists\n",
    "# merged_df columns: ['Date', 'Close', 'Volume', 'Avg_Sentiment', 'Next_Close', 'Target']\n",
    "\n",
    "# Create lagged returns \n",
    "merged_df['Return_1'] = merged_df['Close'].pct_change(1).fillna(0)\n",
    "merged_df['Return_2'] = merged_df['Close'].pct_change(2).fillna(0)\n",
    "merged_df['Return_3'] = merged_df['Close'].pct_change(3).fillna(0)\n",
    "merged_df['Return_4'] = merged_df['Close'].pct_change(4).fillna(0)\n",
    "merged_df['Return_5'] = merged_df['Close'].pct_change(5).fillna(0)\n",
    "\n",
    "# Lagged sentiment \n",
    "merged_df['Sentiment_Lag1'] = merged_df['Avg_Sentiment'].shift(1).fillna(0)\n",
    "merged_df['Sentiment_Lag2'] = merged_df['Avg_Sentiment'].shift(2).fillna(0)\n",
    "merged_df['Sentiment_Lag3'] = merged_df['Avg_Sentiment'].shift(3).fillna(0)\n",
    "merged_df['Sentiment_Lag4'] = merged_df['Avg_Sentiment'].shift(4).fillna(0)\n",
    "merged_df['Sentiment_Lag5'] = merged_df['Avg_Sentiment'].shift(5).fillna(0)\n",
    "\n",
    "#  Moving averages\n",
    "merged_df['MA3'] = merged_df['Close'].rolling(3).mean().fillna(0)\n",
    "merged_df['MA5'] = merged_df['Close'].rolling(5).mean().fillna(0)\n",
    "\n",
    "#  Volatility \n",
    "merged_df['Volatility3'] = merged_df['Close'].rolling(3).std().fillna(0)\n",
    "merged_df['Volatility5'] = merged_df['Close'].rolling(5).std().fillna(0)\n",
    "\n",
    "#  Volume change \n",
    "merged_df['Vol_Change'] = merged_df['Volume'].pct_change().fillna(0)\n",
    "\n",
    "\n",
    "#  Features & Target\n",
    "\n",
    "features = ['Close','Volume','Avg_Sentiment',\n",
    "            'Return_1','Return_2','Return_3','Return_4','Return_5',\n",
    "            'MA3','MA5','Volatility3','Volatility5',\n",
    "            'Sentiment_Lag1','Sentiment_Lag2','Sentiment_Lag3','Sentiment_Lag4','Sentiment_Lag5',\n",
    "            'Vol_Change']\n",
    "\n",
    "X = merged_df[features].copy()\n",
    "y = merged_df['Target']\n",
    "\n",
    "# Replace inf/-inf and NaN\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "#  Train-Test Split (chronological)\n",
    "\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "\n",
    "# 🔹 Feature Scaling\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "#  Train XGBoost Classifier\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "#  Predictions & Evaluation\n",
    "\n",
    "y_pred = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\n Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ConfusionMatrixDisplay(cm).plot()\n",
    "plt.show()\n",
    "\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "print(\"ROC-AUC:\", auc)\n",
    "\n",
    "\n",
    "#  Feature Importance\n",
    "\n",
    "importances = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop Features:\\n\", importances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This code creates lagged price and sentiment features, scales them, and trains multiple ML models (Logistic Regression, SVM, Naive Bayes) to predict next-day Bitcoin price movement using BERT sentiment, then evaluates each model with classification reports, confusion matrices, and ROC-AUC scores.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.690768Z",
     "iopub.status.idle": "2025-10-15T01:19:13.691029Z",
     "shell.execute_reply": "2025-10-15T01:19:13.690914Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.690903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#  Prepare Rolling Features for BERT + ML Models\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure merged_df from your notebook exists\n",
    "# merged_df columns: ['Date', 'Close', 'Volume', 'Avg_Sentiment', 'Next_Close', 'Target']\n",
    "\n",
    "# Create lagged returns \n",
    "for i in range(1, 6):\n",
    "    merged_df[f'Return_{i}'] = merged_df['Close'].pct_change(i).fillna(0)\n",
    "\n",
    "#  Lagged sentiment \n",
    "for i in range(1, 6):\n",
    "    merged_df[f'Sentiment_Lag{i}'] = merged_df['Avg_Sentiment'].shift(i).fillna(0)\n",
    "\n",
    "#  Moving averages \n",
    "merged_df['MA3'] = merged_df['Close'].rolling(3).mean().fillna(0)\n",
    "merged_df['MA5'] = merged_df['Close'].rolling(5).mean().fillna(0)\n",
    "\n",
    "#  Volatility \n",
    "merged_df['Volatility3'] = merged_df['Close'].rolling(3).std().fillna(0)\n",
    "merged_df['Volatility5'] = merged_df['Close'].rolling(5).std().fillna(0)\n",
    "\n",
    "#  Volume change \n",
    "merged_df['Vol_Change'] = merged_df['Volume'].pct_change().fillna(0)\n",
    "\n",
    "\n",
    "#  Features & Target (using BERT sentiment)\n",
    "\n",
    "features = ['Close','Volume','Avg_Sentiment'] + \\\n",
    "           [f'Return_{i}' for i in range(1,6)] + \\\n",
    "           [f'Sentiment_Lag{i}' for i in range(1,6)] + \\\n",
    "           ['MA3','MA5','Volatility3','Volatility5','Vol_Change']\n",
    "\n",
    "X = merged_df[features].copy()\n",
    "y = merged_df['Target']\n",
    "\n",
    "# Replace inf/-inf and NaN\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "#  Train-Test Split (chronological)\n",
    "\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "\n",
    "#  Feature Scaling\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "#  Train & Evaluate Models\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),\n",
    "    'SVM (RBF Kernel)': SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=42),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_prob = model.predict_proba(X_test_scaled)[:,1] if hasattr(model, \"predict_proba\") else y_pred\n",
    "    \n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    ConfusionMatrixDisplay(cm).plot()\n",
    "    plt.title(f\"Confusion Matrix: {name}\")\n",
    "    plt.show()\n",
    "    \n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    print(\"ROC-AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.691955Z",
     "iopub.status.idle": "2025-10-15T01:19:13.692271Z",
     "shell.execute_reply": "2025-10-15T01:19:13.692118Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.692104Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save preprocessed features\n",
    "X.to_csv(\"/kaggle/working/X_features.csv\", index=False)\n",
    "\n",
    "# Save scaler and trained model\n",
    "joblib.dump(scaler, \"/kaggle/working/scaler.pkl\")\n",
    "joblib.dump(xgb_model, \"/kaggle/working/xgb_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTING THE OUTPUT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.693315Z",
     "iopub.status.idle": "2025-10-15T01:19:13.693720Z",
     "shell.execute_reply": "2025-10-15T01:19:13.693558Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.693543Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Load prepared features and model\n",
    "X = pd.read_csv(\"/kaggle/working/X_features.csv\")\n",
    "scaler = joblib.load(\"/kaggle/working/scaler.pkl\")\n",
    "xgb_model = joblib.load(\"/kaggle/working/xgb_model.pkl\")\n",
    "\n",
    "# Latest day features\n",
    "latest_features = X.iloc[[-1]]\n",
    "\n",
    "# Scale and predict\n",
    "latest_features_scaled = scaler.transform(latest_features)\n",
    "next_day_pred = xgb_model.predict(latest_features_scaled)[0]\n",
    "prob_increase = xgb_model.predict_proba(latest_features_scaled)[0][1]\n",
    "\n",
    "# Display result\n",
    "if next_day_pred == 1:\n",
    "    print(f\" Prediction: Bitcoin price will increase tomorrow (Confidence: {prob_increase*100:.2f}%)\")\n",
    "else:\n",
    "    print(f\" Prediction: Bitcoin price will decrease tomorrow (Confidence: {100 - prob_increase*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T01:19:13.694667Z",
     "iopub.status.idle": "2025-10-15T01:19:13.695141Z",
     "shell.execute_reply": "2025-10-15T01:19:13.694975Z",
     "shell.execute_reply.started": "2025-10-15T01:19:13.694955Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Load saved model and scaler\n",
    "scaler = joblib.load(\"/kaggle/working/scaler.pkl\")\n",
    "xgb_model = joblib.load(\"/kaggle/working/xgb_model.pkl\")\n",
    "\n",
    "# Scale all features\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# Predict for all rows\n",
    "predictions = xgb_model.predict(X_scaled)\n",
    "probabilities = xgb_model.predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "# Add predictions to your dataset\n",
    "output_df = X.copy()\n",
    "output_df['Predicted_Movement'] = predictions\n",
    "output_df['Prediction_Confidence'] = probabilities\n",
    "output_df['Prediction_Label'] = output_df['Predicted_Movement'].map({1: 'Increase', 0: 'Decrease'})\n",
    "\n",
    "# Save to CSV\n",
    "output_df.to_csv(\"/kaggle/working/bitcoin_predictions.csv\", index=False)\n",
    "print(\" Predictions saved successfully as 'bitcoin_predictions.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # FINAL CONCLUSION:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.The project demonstrates that Twitter sentiment has some correlation with Bitcoin price movements, but predicting exact direction is challenging.\n",
    "\n",
    "2.Models achieved an accuracy of ~54%, after adding Tuning methods showing slight predictive power above random guessing.\n",
    "\n",
    "Positive sentiment tends to be slightly more indicative of upward price movement, as reflected in recall scores.\n",
    "\n",
    "3.Feature engineering (rolling sentiment averages, price momentum)  have added which  further improved our model performance.\n",
    "\n",
    "4.Limitations include noisy social media data, low signal-to-noise ratio, and highly volatile cryptocurrency market.\n",
    "\n",
    "Future improvements:\n",
    "\n",
    "Fine-tuning BERT on cryptocurrency tweets\n",
    "\n",
    "Using ensemble or deep learning models (LSTM/GRU) for sequential prediction\n",
    "\n",
    "Incorporating additional indicators like trading volume, Google Trends, or Reddit sentiment\n",
    "\n",
    "Takeaway: This project showcases the integration of social media sentiment analysis with financial data for predictive modeling, highlighting the challenges and opportunities of machine learning in volatile markets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Model accuracies\n",
    "models = ['Random Forest', 'XGBoost', 'Logistic Regression', 'SVM', 'Naive Bayes']\n",
    "accuracies = [71, 54, 51, 49, 48]  # Accuracy percentages\n",
    "\n",
    "# Split into positive and negative classes\n",
    "pos_accuracy = [73, 56, 52, 50, 49]  # Positive class accuracy\n",
    "neg_accuracy = [69, 52, 50, 48, 47]  # Negative class accuracy\n",
    "\n",
    "# Create figure and axis with larger size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Set width of bars and positions\n",
    "width = 0.35\n",
    "x = np.arange(len(models))\n",
    "\n",
    "# Create bars\n",
    "plt.bar(x - width/2, pos_accuracy, width, label='Positive Class', color='forestgreen', alpha=0.8)\n",
    "plt.bar(x + width/2, neg_accuracy, width, label='Negative Class', color='crimson', alpha=0.8)\n",
    "\n",
    "# Customize the plot\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks(x, models, rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "# Add value labels on top of bars\n",
    "def add_value_labels(x, values):\n",
    "    for i, v in zip(x, values):\n",
    "        plt.text(i, v + 0.5, str(v)+'%', ha='center', va='bottom')\n",
    "\n",
    "add_value_labels(x - width/2, pos_accuracy)\n",
    "add_value_labels(x + width/2, neg_accuracy)\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "for model, acc, pos, neg in zip(models, accuracies, pos_accuracy, neg_accuracy):\n",
    "    print(f\"\\n{model}:\")\n",
    "    print(f\"- Overall Accuracy: {acc}%\")\n",
    "    print(f\"- Positive Class Accuracy: {pos}%\")\n",
    "    print(f\"- Negative Class Accuracy: {neg}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
